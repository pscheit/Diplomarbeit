\chapter{Konzepte und Implementierung}

\begin{anmerkung}[Funktionen und Methoden]
In PHP sind Variablen immer mit \$ gekennzeichnet. Wenn ich: \\
\code{call(\$p1, \$p2, \$p3)}\\
schreibe, meine ich, dass die Funktion \term{call} die Parameter \term{p1}, \term{p2} und \term{p3} hat.
\end{anmerkung}

\section{Dynamischer Code} \label{dynamischer-code}
Betrachtet man die Besonderheiten von einer Webapplikation in PHP, stellt sich schnell die Frage, wie man eine sowohl sauber abstrahierte als auch komplexe, aber gleichzeitig performante Lösung schreibt. Umfangreicher Code bedeutet viel Aufwand für den Interpreter. Umfangreicher Code entsteht aber dadurch, dass die Applikation eine hohe Abstraktion besitzt. Eine hohe Abstraktion ermöglicht dem Entwickler einfacheres Arbeiten mit den Methoden der Applikation. Er ist flexibler in allen Entscheidungen, die er treffen muss, kann effizienter Aufgaben implementieren und kann sich somit auf die Businesslogik der Applikation konzentrieren. Der Code wird einfacher zu warten und ist deklarativer. \\
In keinem Fall kann man auf umfangreichen Code in einer objektorientierten Applikation aufgrund der Performanz verzichten. In PHP ist es möglich einen sogenannten Bytecode-Cache zu benutzen, der zumindest das wiederholte Parsen des Codes überflüssig macht\footnote{ein solcher Bytecode-Cache ist z.~B. die PECL-Erweiterung: APC}. Leider ist es oft so, dass die benötigte Architektur für einen solchen Bytecode-Cache in normalen Webprojekten nicht zur Verfügung steht. Nur auf eigenen selbstbetreuten Hostinglösungen ist es möglich Bytecode-Caches zu konfigurieren und zu warten. Da die meisten Webseiten auf kommerziellen, einfachen Hostinglösungen liegen, ist dies nicht immer der Fall. \\
Die Lösung des Problems ist in \PSCORM die Möglichkeit, PHP Code dynamisch zu erzeugen. Durch die Konzepte von Groovy (GORM) und Propel \cite{propel}, \cite{orm-dynamic-languages}, kam ich auf die Idee die \ORM-Lösung in zwei Ebenen zu organisieren:\\
Die erste Ebene ist eine komplett abstrahierte, umfangreiche und objektorientierte Applikation. Sie soll die Anforderungen für ein modernes \ORM-Framework erfüllen und flexibles und schnelles Arbeiten ermöglichen. Hier sollen die Mappings erstellt und konfiguriert, Abfragen geschrieben und Modelle definiert werden. \\
Die zweite Ebene wird von der ersten Ebene dynamisch erzeugt. Der PHP Code der ersten Ebene schreibt PHP Code für die zweite Ebene. Damit ist die erste Ebene mit einem Compiler vergleichbar, deshalb das \textbf{C} in \PSCORM. Die Strukturen der ersten Ebene sollen in einfachen Code für die zweite Ebene transformiert werden. Die zweite Ebene wird dann auf den Webserver geladen und funktioniert ohne den umfangreichen Code. Jediglich der schlanke Core wird von beiden Ebenen benutzt.\\

\begin{anmerkung}[Laufzeit und Entwicklungszeitpunkt]
Aktionen die in der ersten Ebene ausgeführt werden, bezeichne ich auch als „Aktionen zum Entwicklungszeitpunkt“ oder „Aktionen beim Kompilieren“. \\
Wenn ich von Algorithmen spreche, die „zur Laufzeit“ aufgerufen werden, meine ich Algorithmen, die durch Code in der zweiten Ebene umgesetzt werden.
\end{anmerkung}

\section{Metainformationen}

Zu jeder Klasse muss eine Menge an zusätzlichen Informationen bereitgestellt werden. Der Entwickler muss diese Metainformationen ergänzen können, so dass er das Verhalten von \PSCORM kontrollieren kann. In den Metainformationen befinden sich z.~B.:
\begin{itemize}
\item der Name der Relation in dem sich die Datensätze befinden
\item die Spalten der Relation und ihre Typen
\item die Mappings zu den einzelnen Spalten
\item Beziehungen zu anderen Objekten, inklusive \term{Lazy Loading}-Einstellungen und Constraints für Updates und Deletes
\item Informationen über die Vererbung und die Klassenhierarchie der Klasse
\item Name und Eigenschaften des Primärschlüssels
\end{itemize}
So wie in Doctrine könnte das Format, in dem der Entwickler diese Informationen an das System übergibt, variabel sein. In Doctrine sind XML, YAML und Annotations (formatierte Kommentare in der Klasse selbst) möglich. Da in der zweiten Ebene diese Informationen in einfach ausführbaren Code umgewandelt werden können, sind die Voraussetzungen für verschiedene Metadatenformate gegeben. Bis jetzt gibt es nur einen Prototypen einer PHP API. Die Metainformationen sollten aber nicht in der Entity-Klasse selbst untergebracht werden, da sie sonst mit den Businessinformationen des Objektes kollidieren könnten. In der Demo-Applikation für die Evaluation ist dies noch für den Namen der Tabelle der Fall. Im Moment wird dieser im Objekt selbst gespeichert, da er die einzige gebrauchte Metainformation in der Demo ist. Dies soll in Zukunft geändert werden. 

\section{Cache} \label{konzept-cache}

Es ist unmöglich auf einen \objectcache in einer \ORM-Software zu verzichten. Er bringt nicht nur einen enormen Performanzvorteil, sondern löst auch automatisch das Problem der Ob\-jekt\-iden\-ti\-fi\-zie\-rung (Kapitel \ref{object-loading}). Durch das Cache-Kriterium, welches entscheidet, ob ein Objekt bereits geladen wurde, ist notwendigerweise jedes Objekt eindeutig identifizierbar. Die Applikation muss also darauf achten, dass jedes Objekt eine OID besitzt. Dies ist einfach zu gewährleisten, indem immer ein künstlicher Integer-Schlüssel in die Tabelle eingefügt wird, wenn es keine eindeutige OID gibt. Dieser wird dann für jede neue Zeile automatisch erhöht und ist immer eindeutig für alle Objekte dieser Klasse. Wenn eine Abfrage oder eine \term{Lazy Loading}-Methode nun versucht ein Objekt zu instanziieren, befragt es immer erst den Cache. Die Frage ist ein Methodenaufruf mit 2 Parametern: Der erste Parameter ist die Klasse des gewünschten Objektes und der zweite Parameter die OID. Der \objectcache verwaltet somit alle Referenzen auf alle Objekte in der Applikation und sollte überall erreichbar sein. Denn immer wenn ein neues Objekt aus der Datenbank geladen wird, sollte dies dem Cache mitgeteilt werden, damit alle Teile der Applikation vom Cache profitieren können. Vielmehr darf es nicht erlaubt sein, ein Objekt zu erzeugen, ohne, dass der Cache darüber informiert wird, denn sonst wäre es möglich, dass zwei nicht identische Objekte dieselbe OID in der Applikation besitzen. \\
In \PSCORM wird der Cache als Singleton implementiert. Er kann überall aus der Applikation heraus mit \code{Cache::getInstance()} geladen werden. Objekte werden mit \code{populate()} dem Cache bekannt gemacht und können mit \code{get(\$class, \$oid)} abgefragt werden. \\
Die Methode mit der von überall ein einzelnes Objekt geladen werden kann, fragt immer zuerst beim Cache nach, ob das Objekt existiert, ansonsten setzt es eine Abfrage an die Datenbank ab. Die Funktion die also überall benutzt wird, um eine Instanz einer Klasse zu erhalten, lautet \code{ORMObject::load(\$class, \$oid)}. \\

\section{Datenbankabstraktion}

Über die Notwendigkeit eines DBAL\footnote{\term{Database Abstraction Layer}} wurde schon vielfach diskutiert \cite{ambler-persistence-layer}. Dennoch sollte man die Vorteile nutzen, die man hat, wenn man das unterliegende Datenbanksystem gut kennt. Natürlich ist es praktisch jederzeit eine Datenbank gegen eine andere (möglicherweise schnellere oder günstigere) auszutauschen, ohne die Applikation zu verändern. Ich denke aber, dass dies eher ein weniger genutzter Sonderfall ist\footnote{In Analogie dazu, dass \RDBMSs auch nicht durch \ODBMSs ersetzt wurden, eben weil große Firmen nur ungern ihr \DBMS wechseln}. Man sollte diesen Punkt verinnerlichen und berücksichtigen, während man seine Applikation entwirft, aber dies sollte sich nicht auf Kosten der Performanz geschehen. \\
In PHP ist in den neueren Versionen (ab PHP5) der notwendige Schritt im PHP-Core gemacht worden: PDO ist eine PHP-Erweiterung die es ermöglicht mit demselben Interface verschiedene Datenbanktypen anzusprechen (Postgres, Oracle, MySQL, MS SQL, DB2, ODBC, SQLLite, Informix). Die Basisklasse der Extension kann mit einer eigenen Klasse abgeleitet werden, so dass die API sogar gekapselt werden kann. PDO soll genauso schnell wie die nativen Treiber der Datenbanken für PHP sein. D.~h. es ist für die Datenbankabstraktion völlig ausreichend seine Abfragen über PDO abzusetzen. \\ In \PSCORM übernimmt die Klasse DB die Ableitung von PDO. Dabei wurde nur der Konstruktor überschrieben, der die Zugangsdaten zur Datenbank aus einer Konfigurationsdatei liest und die allgemeine Query-Funktion um das Loggen von SQL Befehlen zu ermöglichen.

\section{Abbildung von Klassen und Attributen}

Am besten haben mir die Lösungen von TopLink und Doctrine für die Zuordnung von Attributen zu Objekteigenschaften gefallen. Natürlich werden in auch \PSCORM Klassen als Relationen, und Spalten von Tabellen als Objekteigenschaften abgebildet. Jede Klasse, die ein abstraktes Interface \term{ObjectMapping} implementiert, soll dafür genutzt werden ein Mapping zu modellieren. Das Interface benötigt dafür nur zwei Hauptfunktionen: \code{convertToPHPValue()} und \code{convertToDatabaseValue()}. In Doctrine ist ein Mapping in dieser Form umgesetzt worden. Ein Standardset von Basismappings für Integer, String, Datetime, Floats, Decimals usw wird direkt mitgeliefert. So soll es auch in \PSCORM sein. Der einzige Unterschied ist, dass beim Kompilieren (Exportieren) des Projektes nicht die komplette abstrake Mappingklasse in die Applikation eingefügt wird, sondern allein der Code aus den beiden Funktionen zur Umwandlung direkt in die betreffenden Objekte kopiert wird. In der Praxis gibt es dabei ein paar Besonderheiten zu beachten, die ich zum Zeitpunkt dieser Arbeit noch nicht ganz lösen konnte. So war es z.~B. ein Problem, dass der Code nicht auf Teile des Objektes zugreifen darf, weil es in der exportierten Applikation nicht im Kontext eines \term{ObjectMapping} sondern direkt im betreffenden Datenobjekt aufgerufen wird. Mit PHP 5.3 werden aber Closures\footnote{anonyme Lambda Funktionen} eingeführt, die höchstwahrscheinlich das Problem mit nativen PHP Sprachelementen lösen können. \\

\section{Abbildung von Vererbung}

Bei der Abbildung der Vererbung musste zuerst entschieden werden, welche Vererbungstypen in \PSCORM umgesetzt werden sollen. Ich habe zuerst nur vertikales und horizontales Mapping sowie Filter-Mapping implementiert. Ich hatte überlegt generelles Mapping zusätzlich zu implementieren. Das Problem beim generellen Mapping ist aber, dass es sehr schnell unperformant wird, da sehr viele Joins zum Laden eines Objektes in der Abfrage nötig sind. Dieses Verfahren hätte aber gut genutzt werden können, um sehr kleine Projekte mit geringem Umfang on-the-fly persistent machen zu können. Durch einen Test hat sich aber gezeigt, dass es schneller ist, das Objekt serialisiert\footnote{bei PHP für alle Objekte möglich} in eine Tabelle mit BLOBs zu speichern, mit den Primärschlüsseln \textit{OID} und \textit{Klasse}. Die Struktur ähnelt etwas einem \objectcache ist aber persistent in der Datenbank. Zur Abfrage eines Objektes wird nur eine Abfrage benötigt. Zur Performanz-Optimierung könnte man noch für jede Klasse so eine Tabelle anlegen, so dass der zweite Primärschlüssel \textit{Klasse} wegfallen würde. \\
Die einfachste Form bei der Implementierung der Vererbung ist nicht das horizontale Mapping, sondern das Filter-Mapping. Beim Filter-Mapping gibt es nicht viele Probleme zu lösen. In den Metainformationen zur Klasse wird der Name der Klassentabelle und der Filter-Spalte (Discriminator) definiert. Durch die dynamische Code-Generierung, kann die \code{save()}- und \code{get()}-Methode der Entity-Klasse schon so modifiziert werden, dass diese die Abfrage an die Klassentabelle stellt und den Discriminator immer als Filter im WHERE-Teil der Abfrage benutzt. Beim Speichern (und beim Laden) des Objektes wird auch \code{save()} vom Elternobjekt aufgerufen, so dass Änderungen von Informationen für Spalten, die nur im Elternobjekt existieren, ebenfalls gespeichert werden. Dieser rekursive Aufruf setzt sich dann bis zur ersten Klasse der Hierarchie fort, da das Elternobjekt wiederum die Methode seines Elternobjekts aufruft. Bei einer tiefen Klassenhierarchie könnte dies ein Performanzproblem beim Speichern geben. Dann müsste man im rekursiven Aufruf die Updates der einzelnen Klassen sammeln und diese in einem Update an die Datenbank schicken. Dazu wäre eine größere Modifikation der \code{save()}-Methoden in allen Klassen nötig, aber es wäre nicht unmöglich.\\
Der Vorteil beim Filter-Mapping ist, dass alle OIDs einer Klassenhierarchie für ein Objekt gleich sind. Da ein Datensatz für ein Objekt nur eine Zeile in einer Relation ist, sind alle Datensätze der Unterklassen von diesem Primärschlüssel abhängig. Jedes Objekt kennt also beim Speichern seine OID und kann somit einfach ein 
\lstset{style=SQL}
\begin{lstlisting}
UPDATE :filter-tabelle SET attribute1 = attribute1wert, ... 
   WHERE id = :OID
\end{lstlisting}
\noindent ausführen, und so seine Spalteninformationen aktualisieren. Das ist bei horizontalem und vertikalem Mapping nicht gegeben. Dort ist es nämlich so, dass der Datensatz für ein Objekt immer auf mehrere Tabellen verteilt ist, sofern das Objekt mindestens ein Elternobjekt besitzt. Beim Aufrufen der \code{get()}-Funktion wird auch die \code{get()}-Funktion des Elternobjektes aufgerufen. Doch wann soll dies in der \code{get()}-Funktion des Kindes passieren? Passiert dies vor dem Initialisieren des Kindobjektes, überschreibt die Elternklasse zumindest das Attribut OID in der Kindklasse, d.~h. die Kindklasse kennt seine eigene OID nicht mehr. Werden die Funktionsaufrufe umgedreht (dies ist in Java z.~B. gar nicht möglich) wird das Attribut für die OID der Elternklasse überschrieben. D.~h. entweder muss garantiert werden, dass jede OID der Klassenhierarchie einen eindeutig unterscheidbaren Namen hat oder aber die OIDs müssen noch separat abgesichert werden. Ersteres wäre natürlich für die Applikation das eleganteste, aber für den Datenbankdesigner ein großes Hinderniss, wenn man bedenkt, dass eine Klasse auch in mehreren verschiedenen Klassenhierarchien existieren kann. Der beste Ansatz ist also in einem Stack (Array) die OIDs separat beim Aufrufen der jeweiligen \code{init()}-Funktion abzuspeichern. Von diesem Stack kann dann jede Klasse bei \code{save()} seine OID herunternehmen und so seinen UPDATE-Befehl ausführen. \\
Das Vorgehen ist für horizontales Mapping und vertikales Mapping  gleich, da beim horizontalen Mapping zwar Attribute doppelt aktualisiert werden, dies aber nicht weiter schlimm ist\footnote{Höchstens bei einer tiefen Klassenhierarchie, aber dieses Problem könnte man auch Analog wie das Problem für das Filter-Mapping beheben}. \\
Lediglich die Beziehungen zwischen den Objekten müssen noch gepflegt werden. Diese Vererbungsbeziehung wird als 1:1-Beziehung definiert und als gewöhnliche Beziehung behandelt. Die Konzepte dazu werden im nächsten Kapitel erläutert. \\

\section{Beziehungen zwischen Objekten}

Es soll in \PSCORM zum Entwicklungszeitpunkt festgelegt werden können, welche Abhängigkeiten eine spezielle Klasse zu anderen Klassen hat. Dabei soll entschieden werden, ob \PSCORM die Funktionen zur Verwaltung einer Collection selbst hinzufügen soll. Dies ist möglich, da die erste Code Ebene auch neue Eigenschaften zu Objekten hinzufügen kann. Wenn gewünscht, übernimmt auch der automatisch erzeugte Code die Pflege der Beziehung beim Speichern des Objektes (Delete, Update, Insert). Eine Beziehung zwischen Objekten kann somit unidirektional und bidirektional spezifiziert werden, indem entweder beide Seiten oder nur eine Seite die Metainformationen für die Beziehung angeben. \\
Der dynamische Code ermöglicht hier eine eigentlich komplexe Implementierung einer transparenten Collection als PHP-Basisstruktur (Array). Normalerweise müsste ein Array durch ein Objekt gekapselt werden. Denn immer wenn Objekte der Collection hinzugefügt werden oder entfernt werden, muss auch die Beziehung in der Datenbank gepflegt werden. Durch \PSCORM kann aller Code der zum Verwalten der Collection benötigt wird, direkt in die Entity-Klasse geschrieben werden. In Doctrine muss dafür eine komplexe Datenstruktur benutzt werden, was den Nachteil hat, dass die sehr performanten, in PHP eingebauten Array-Funktionen nicht mehr benutzt werden können. Gerade bei großen Datenmengen ist in PHP nichts performanter als ein einfacher Array. Durch die dynamisch angelegten Methoden wird die Kapselung des Arrays direkt in der Klasse erreicht. \\
Für jede Beziehung soll es möglich sein, den Fremdschlüssel im relationalen Schema auf der Many-Seite der Beziehung einzufügen oder eine Zwischentabelle zu benutzen, wie in \cite{lodhi-ghazali} beschrieben.

\section{Abfragesprache}

Die Entscheidung, wie Abfragen für Mengen von Objekten an das System gestellt werden (siehe Kapitel \ref{marshalling}), war nicht besonders leicht. Eine der Anforderungen, die ich beim Arbeiten mit Kohana für am wichtigsten hielt war, dass die Anfragesprache vollständig sein muss. Es ist ein Problem, wenn der Entwickler seine Anfrage an die Datenbank in SQL formulieren kann, aber keine Möglichkeit hat diese Anfrage an die \ORM-Software zu stellen. Eine eigene Anfragesprache zu schreiben war für den ersten Entwurf von \PSCORM eine viel zu große Aufgabe. Die Entscheidung fiel dann letzendlich auf eine eigene Abfragesprache, jedoch sollte das Schreiben von SQL möglich sein. Neben der Bindung an das Schema der Datenbank ist jedoch das größte Problem, dass das Schreiben von SQL sehr fehleranfällig ist. Denkt man insbesondere an \term{SQL-Injections}, ist meist ein Entwicklerfehler für unsicheren oder fehlerhaften Code verantwortlich. Mein Focus bei der Entwicklung des Abfragesystems von \PSCORM lag deshalb darauf, alles zu ermöglichen, aber alle Standardarbeiten vom System übernehmen zu lassen:
\begin{itemize}
\item Joins können mit einer Funktion erstellt werden, so dass nicht die komplette ON-Bedingung geschrieben werden muss.
\item Das Escaping\footnote{auch Schutz vor \term{SQL-Injection}} von Datentypen kann durch eine Spracherweiterung\footnote{z.~B. Umschließen des Wertes mit geschweiften Klammern} des SQLs automatisch erledigt werden.
\item Die Spalten eines Objektes müssen nicht explizit referenziert werden\footnote{z.~B. SELECT \{:object\} FROM :tabelle}
\end{itemize}
Der Vorteil von \PSCORM ist, dass keine noch so aufwendige Funktion, die für das Parsen von handgeschriebenem SQL benötigt wird oder für das Überprüfen vieler zusätzlicher Bedingungen beim Erstellen eines SQL-Befehls zuständig ist, sich auf die Laufzeit der Applikation auswirken kann. So könnten z.~B. in Zukunft handgeschriebene SQL-Befehle gegen das Datenmodell geprüft werden, Sicherheitsaspekte behandelt oder Optimierungstipps gegeben werden. Bei der Kompilierung der Applikation wird dann reines SQL exportiert, welches sicherer und fehlerunfällliger sein wird. \\
Zusätzlich zu der Unterstützung für das Schreiben von eigenem SQL gibt es Query-Klassen die zur Laufzeit genutzt werden können. Diese Query-Klassen sind erweiterbar und bieten einen Grundstock an Standardfunktionen. Somit gibt es eine Funktion für jedes Entity-Objekt, welche die ganze Relation als Objekte hydriert.\footnote{z.~B. für \object{Project}: \code{Query::getALLProjects()}}. Diese Query-Klassen können dann von mehreren Stellen der Applikation auch außerhalb eines Objekt-Kontext benutzt werden. In den Tests für die Evaluation (Kapitel \ref{evaluation}) werden zwei Beispiele für Query-Klassen gezeigt.\\

\section{Object Loading}

Wie das generelle Object-Loading einzelner Objekte in \PSCORM funktioniert, wurde zum Teil schon im Abschnitt über den Cache beschrieben (Kapitel \ref{konzept-cache}). \\
Im Kapitel \ref{object-loading} über das \term{Object Loading} wurde ein Ansatz vorgestellt für die Optimierung von Ladestrategien. Eine Idee für ein Statistiksystem, welches Tipps geben kann, welche Strategie gerade die sinnvollste wäre, möchte ich hier vorstellen. Es soll sich um das Problem handeln, wie man sich entscheidet eine Menge von Unterobjekten in einer Applikation zu laden. Wie zuvor schon erwähnt, gibt es zwei Strategien Objekte zu laden:
\begin{enumerate}
\item Alle Datensätze der Tabelle werden zuvor mit einer Abfrage geladen und als Objekte im Cache referenziert (\term{Prefetching})
\item Die Datensätze werden beim Zugriff auf die Unterobjekte durch die Applikation einzeln, durch simple Abfragen geladen und zu Objekten transformiert (\term{Lazy Loading})
\end{enumerate}
Bei der ersten Mögichkeit werden also immer alle Datensätze der gesamten Relation geladen. Bei der zweiten Möglichkeit werden nur die Datensätze geladen, auf die durch die Applikation tatsächlich zugegriffen wird. \\
Zur Veranschaulichung noch ein Beispiel: Es ist bekannt, dass in einer Applikation auf 140 Objekte vom Typ \object{Project} aus der Tabelle \tabelle{projects} zugegriffen wird. Die Tabelle \tabelle{projects} hat eine Kardinalität von 182. Für welche Möglichkeit sollte sich der Entwickler entscheiden? \\
Verwendet man die \term{Prefetching}-Methode, würde dies bedeuten zuerst die Abfrage \code{SELECT * FROM projects} auszuführen und die daraus enstehenden Objekte mit populate() dem Cache bekannt zu machen. Werden dann im weiteren Verlauf in der Applikation die 140 \object{Project}-Objekte referenziert, werden diese aus dem Cache geladen. Weitere Anfragen an die Datenbank entfallen. \\
Benutzt man die \term{Lazy Loading}-Methode, wäre keine Abfrage auszuführen. Die Objekte werden in der Applikation mit \code{ORMObject::load()} geladen. Sofern sich die Objekte dann noch nicht im Cache befinden, wird eine einzelne Abfrage an die Datenbank gestellt [\ref{konzept-cache}]. Dann werden natürlich 140 Abfragen der Form \code{SELECT * FROM projects WHERE id = :id} an die Datenbank gesendet. \\
In diesem speziellen Beispiel müsste sich der Entwickler für \term{Prefetching} entscheiden, wenn er die schnellere Methode wählen will. Wie gelangt man aber zu so einer Entscheidung? Exakter formuliert müsste man fragen: ab wann lohnt es sich \term{Prefetching} statt \term{Lazy Loading} zu benutzen? Oder: Wann ist \term{Lazy Loading} langsamer als \term{Prefetching}? Ich kann zum Ende dieser Arbeit noch keine geschlossene, mathematische Formel angeben, dennoch lassen sich die Variablen dieser Formel gut bestimmen. \\
Die Kardinalität der Domain, aus der die Objekte kommen, ist ausschlaggebend. In diesem Beispiel hat \tabelle{projects} 182 Datensätze, die ein Objekt repräsentieren. Würden in der Applikation auf alle \object{Project}-Objekte zugegriffen, leuchtet ein, dass \term{Prefetching} die beste Methode ist, denn die \term{Roundtrips} zur Datenbank werden minimiert [\ref{evaluation}]. Wird nur eine sehr geringe Anzahl von \object{Project}-Objekten geladen (Anzahl der Objekte \mth{\ll} Kardinalität der Tabelle) leuchtet ein, dass \term{Lazy Loading} benutzt werden sollte. Ich bezeichne die gesamte Anzahl der Datensätze in der Datenbank als x.\\
Ein weiterer Faktor ist die Größe der einzelnen Datensätze. Bei der kleinen Größe eines Datensatzes der Relation \tabelle{projects} im Beispieldatenbankschema (Anhang \ref{beispiel-datenbankschema}), welcher aus 3 Integers und einem Varchar besteht, war der Unterschied zwischen den beiden Methoden in einem Benchmark schwer zu erkennen. Deshalb wurde die Tabelle für \tabelle{projects} mit einer weiteren Spalte \term{data} ergänzt. \term{data} ist ein Largeblob und wurde mit verschieden großen, Binäredaten befüllt, so dass jeder Datensatz nun mindestens die Größe des Largeblob hat. Diese Veränderung der Datengröße hat zur Folge, dass nun nicht mehr die \term{Roundtrips} and die Datenbank der dominierende Zeitfaktor sind, sondern eine Abfrage eines einzelnen Datensatzes viel Zeit kostet. \\
Es wurde dann mit unterschiedlichen Größen des Largeblobs (256 KB, 512KB, 4MB, 16MB) der folgende Test durchgeführt: Es wurde die Laufzeit für die \term{Prefetching}-Methode ausgewertet, wenn in der Applikation alle Objekte, die in der Datenbank existierien, angefragt werden (x Objekte). Danach wurde die \term{Lazy Loading} Methode mit einer variablen Anzahl y von Objekten solange laufen gelassen, bis die vorher ermittelte Laufzeit der \term{Prefetching}-Methode erreicht wurde. Das Ergebnis dieses Versuches ist also, dass in derselben Laufzeit mit der \term{Prefetching}-Methode x Objekte und mit der \term{Lazy Loading}-Methode y Objekte geladen werden können. \\
Das Verhältnis zwischen x und y veränderte sich, wenn die Menge der Daten im Feld \term{data} vergrößert oder verkleinert wurde. Natürlich ist dies nur ein experimentelles Ergebnis, aber es lässt sich eine Tendenz ablesen: Desto größer der Datensatz wurde, desto mehr Objekte konnten mit \term{Lazy Loading} in der selben Zeit geladen werden. Das bedeutet, dass es sich bei größeren Datensätzen lohnt vermehrt \term{Lazy Loading} zu benutzen, als die kompletten Daten mit \term{Prefetching} zu laden. Dies ist auch das logisch erwartete Ergebnis: Sind die Datensätze sehr klein, dominiert der Overhead der \term{Roundtrips} an die Datenbank die Laufzeit und es ist sinnvoller direkt alle Daten auf einmal zu laden. Sind die Datensätze groß, möchte man möglichst wenig nicht genutzte Daten übertragen. \\
Für \PSCORM könnte dann eine Strategie zur Optimierung sein, dass dieses Verhältnis zwischen y und der Kardinalität der Tabelle dieser Objekte dynamisch errechnet wird. Dieses Verhältnis muss dann zusätzlich von der Datensatzgröße beeinflusst werden. Erhält man z.~B. ein Verhältnis von 73\% so bedeutet das, dass in der Applikation mindestens 73\% der Datensätze der Tabelle als Objekte angefordert werden müssen, damit sich \term{Prefetching} lohnt. Werden die Datensätze größer, so muss das Verhältnis weiter ansteigen, denn für große Datensätze ist der dominierende Zeitfaktor die Übertragung zum Client, also wird die Menge der übertragenen Daten minimiert\footnote{Es werden nur die Daten abgefragt, die in der Applikation geliefert werden müssen}. Werden die Datensätze kleiner, wird das Verhältnis kleiner. Es dann schneller die Anzahl der Anfragen zu minimieren, da der Overhead eines \term{Roundtrips} die Laufzeit dominiert und es ist nicht mehr so relevant, wieviele Daten gelesen werden. Es können also Daten zusätzlich geladen werden, die in der Applikation nicht benötigt werden. \\
Diese Berechnungen sollen bei der Entwicklung der Applikation bereits geschehen, so dass der Entwickler -- sofern er mit guten Testdaten arbeitet -- früh Tipps erhalten kann\footnote{z.~B. über eine Entwicklerkonsole}, wie er das Object-Loading verbessern kann. \\
Ein praktisches Problem ist dabei die Berechnung von y, da natürlich keine Laufzeittests bei der Entwicklung von Hand durchgeführt werden können. Mein naiver Ansatz war ein Basisverhältnis von 60\%\footnote{geschätzt durch Experimente} anzunehmen und dieses durch Modifikationen wie eine übergroße Datensatzgröße oder die Kardinalität der Tabelle im Verhältnis zur Anzahl der zu ladenen Objekte in der Applikation zu beeinflussen. \\
Es ist auch noch nicht klar, ob noch weitere wichtige Faktoren das Laden von Objekten stark beeinflussen können, die ich hier noch nicht überblicken konnte.\\
